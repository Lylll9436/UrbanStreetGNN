"""
è¾¹åµŒå…¥æ¨¡å‹è®­ç»ƒè„šæœ¬
åŸºäºGNNå»ºæ¨¡ç­–ç•¥çš„å®Œæ•´å®ç°
"""

import pickle
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
import networkx as nx
import numpy as np
import random
from edge_embedding import EdgeEmbeddingModel, ContrastiveLearningLoss, GraphAutoEncoderLoss
import os
from datetime import datetime


def encode_highway_type(highway_type):
    """ç¼–ç é“è·¯ç±»å‹"""
    type_mapping = {
        'footway': 0, 'secondary': 1, 'tertiary': 2, 'primary': 3,
    }
    return type_mapping.get(highway_type, 0)


def extract_node_centrality(nx_graph):
    """æå–èŠ‚ç‚¹ä¸­å¿ƒæ€§ç‰¹å¾"""
    # è®¡ç®—å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡
    degree_centrality = nx.degree_centrality(nx_graph)
    betweenness_centrality = nx.betweenness_centrality(nx_graph)
    closeness_centrality = nx.closeness_centrality(nx_graph)
    
    centrality_features = {}
    for node in nx_graph.nodes():
        centrality_features[node] = [
            degree_centrality.get(node, 0),
            betweenness_centrality.get(node, 0),
            closeness_centrality.get(node, 0)
        ]
    
    return centrality_features


def convert_ego_graphs_to_pytorch(pkl_path):
    """å°†ego-graphsè½¬æ¢ä¸ºPyTorch Geometricæ ¼å¼"""
    print("åŠ è½½pklæ–‡ä»¶...")
    with open(pkl_path, 'rb') as f:
        ego_graphs = pickle.load(f)
    
    print("è½¬æ¢å›¾æ•°æ®...")
    pytorch_graphs = []
    
    for i, graph_data in enumerate(ego_graphs):
        nx_graph = graph_data['graph']
        node_mapping = {node: idx for idx, node in enumerate(nx_graph.nodes())}
        
        # æå–èŠ‚ç‚¹ä¸­å¿ƒæ€§ç‰¹å¾
        centrality_features = extract_node_centrality(nx_graph)
        
        # èŠ‚ç‚¹ç‰¹å¾ï¼šåº¦æ•° + ä¸­å¿ƒæ€§
        node_features = []
        for node in nx_graph.nodes():
            # åŸºç¡€ç‰¹å¾
            degree = nx_graph.nodes[node]['degree']
            centrality = nx_graph.nodes[node]['centrality']
            
            # ç»„åˆç‰¹å¾
            features = [
                degree,  # åº¦æ•°
                centrality,  # åŸå§‹ä¸­å¿ƒæ€§
            ]
            node_features.append(features)
                
        # è¾¹ç‰¹å¾å’Œç´¢å¼•
        edge_features = []
        edge_indices = []
        for u, v, data in nx_graph.edges(data=True):
            # å®‰å…¨åœ°æå–è¾¹ç‰¹å¾ï¼Œå¤„ç†å¯èƒ½çš„å­—ç¬¦ä¸²ç±»å‹
            try:
                highway_encoded = encode_highway_type(data.get('highway', 'unknown'))
                width = float(data.get('width', 0.0))
                length = float(data.get('length', 0.0))
                
                features = [
                    highway_encoded,
                    width,
                    length
                ]
                edge_features.append(features)
                edge_indices.append([node_mapping[u], node_mapping[v]])
            except (ValueError, TypeError) as e:
                print(f"è­¦å‘Šï¼šè·³è¿‡è¾¹ ({u}, {v})ï¼Œæ•°æ®è½¬æ¢é”™è¯¯: {e}")
                print(f"è¾¹æ•°æ®: {data}")
                continue
        
        # åˆ›å»ºDataå¯¹è±¡
        data = Data(
            x=torch.tensor(node_features, dtype=torch.float),
            edge_index=torch.tensor(edge_indices, dtype=torch.long).t().contiguous(),
            edge_attr=torch.tensor(edge_features, dtype=torch.float),
            graph_id=graph_data.get('id', i)
        )
        pytorch_graphs.append(data)
        
        if (i + 1) % 10 == 0:
            print(f"å·²å¤„ç† {i + 1}/{len(ego_graphs)} ä¸ªå›¾")
    
    print(f"è½¬æ¢å®Œæˆï¼å…±å¤„ç† {len(pytorch_graphs)} ä¸ªå›¾")
    return pytorch_graphs


def create_model_config():
    """æ ¹æ®å»ºæ¨¡ç­–ç•¥åˆ›å»ºæ¨¡å‹é…ç½®"""
    config = {
        # è¾“å…¥ç‰¹å¾ï¼šèŠ‚ç‚¹ä¸­å¿ƒæ€§ + è¾¹å±æ€§
        'node_features': 2,      # åº¦æ•°ã€ä¸­å¿ƒæ€§ 
        'edge_features': 3,      # é“è·¯ç±»å‹ã€å®½åº¦ã€é•¿åº¦
        
        # ç¼–ç å±‚ï¼šLinear/MLP(2å±‚)
        'hidden_dim': 256,       # éšè—å±‚ç»´åº¦
        'embedding_dim': 128,    # åµŒå…¥ç»´åº¦
        
        # GNNå±‚ï¼šGCNConv + ReLU + Dropout
        'num_layers': 3,         # GNNå±‚æ•°
        'dropout': 0.2,          # Dropoutç‡
        'conv_type': 'gcn',      # å·ç§¯ç±»å‹
        
        # # æ¶ˆæ¯ä¼ æ’­æ–¹å¼ï¼šæ˜ç¡®åˆ†ç¦»N2Nå’ŒN2E
        # 'message_passing': {
        #     'n2n_layers': 3,     # Node-to-Nodeå±‚æ•°
        #     'n2e_method': 'concatenate',  # Node-to-Edgeèšåˆæ–¹æ³•
        #     'conv_type': 'gcn'   # å·ç§¯ç±»å‹
        # },
        
        # # è¾“å‡ºæ–¹å¼ï¼šç›´æ¥ä½œä¸ºembedding
        # 'output_method': 'direct_embedding'
    }
    return config


def split_data(graphs, train_ratio=0.8, test_ratio=0.2):
    """åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
    total = len(graphs)
    train_size = int(total * train_ratio)
    
    # éšæœºæ‰“ä¹±æ•°æ®
    random.shuffle(graphs)
    
    train_graphs = graphs[:train_size]
    test_graphs = graphs[train_size:]
    
    print(f"æ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(train_graphs)} ä¸ªå›¾")
    print(f"  æµ‹è¯•é›†: {len(test_graphs)} ä¸ªå›¾")
    
    return train_graphs, test_graphs

def train_model(train_graphs, test_graphs, config, num_epochs=500, learning_rate=1e-3, device='cpu'):
    """è®­ç»ƒæ¨¡å‹"""
    print("åˆ›å»ºæ¨¡å‹...")
    model = EdgeEmbeddingModel(**config)
    model = model.to(device)
    
    # æŸå¤±å‡½æ•°ï¼šå›¾è‡ªç¼–ç å™¨æŸå¤±
    gae_loss = GraphAutoEncoderLoss(alpha=0.7, beta=0.3)
    
    # Optimizerï¼šAdam + LR 1e-3
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # L2æ­£åˆ™åŒ–
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10, verbose=True
    )
    
    # æ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(train_graphs, batch_size=1, shuffle=True)
    test_dataloader = DataLoader(test_graphs, batch_size=1, shuffle=False)
    
    print("å¼€å§‹è®­ç»ƒ...")
    model.train()
    
    best_loss = float('inf')
    training_history = []
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0.0
        num_train_batches = 0
        
        for batch in train_dataloader:
            batch = batch.to(device)
            
            # å‰å‘ä¼ æ’­
            center_edge_embedding, structural_score = model(batch)
            
            # è®¡ç®—å›¾è‡ªç¼–ç å™¨æŸå¤±
            loss = gae_loss(center_edge_embedding, batch.edge_index, structural_score)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_train_loss += loss.item()
            num_train_batches += 1
        
        # æµ‹è¯•é˜¶æ®µ
        model.eval()
        total_test_loss = 0.0
        num_test_batches = 0
        
        with torch.no_grad():
            for batch in test_dataloader:
                batch = batch.to(device)
                
                # å‰å‘ä¼ æ’­
                center_edge_embedding, structural_score = model(batch)
                
                # è®¡ç®—æŸå¤±
                loss = gae_loss(center_edge_embedding, batch.edge_index, structural_score)
                
                total_test_loss += loss.item()
                num_test_batches += 1
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_train_loss = total_train_loss / num_train_batches
        avg_test_loss = total_test_loss / num_test_batches
        training_history.append(avg_train_loss)
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆåŸºäºæµ‹è¯•æŸå¤±ï¼‰
        scheduler.step(avg_test_loss)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºæµ‹è¯•æŸå¤±ï¼‰
        if avg_test_loss < best_loss:
            best_loss = avg_test_loss
            torch.save(model.state_dict(), 'best_edge_embedding_model.pt')
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        if (epoch + 1) % 10 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, LR: {current_lr:.6f}")
    
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³æµ‹è¯•æŸå¤±: {best_loss:.4f}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    np.save('training_history.npy', np.array(training_history))
    
    return model, training_history, test_graphs


def evaluate_model_performance(model, graphs, device='cpu'):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    
    all_center_edge_embeddings = []
    all_structural_scores = []
    
    with torch.no_grad():
        for graph in graphs:
            graph = graph.to(device)
            center_edge_embedding, structural_score = model(graph)
            
            all_center_edge_embeddings.append(center_edge_embedding.cpu())
            all_structural_scores.append(structural_score.cpu())
    
    # åˆå¹¶ç»“æœ - ç°åœ¨æ¯ä¸ªå­å›¾è¾“å‡ºä¸€ä¸ªä¸­å¿ƒè¾¹çš„embedding
    all_center_edge_embeddings = torch.cat(all_center_edge_embeddings, dim=0)  # [num_graphs, embedding_dim]
    all_structural_scores = torch.cat(all_structural_scores, dim=0)  # [num_graphs]
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_graphs': len(all_center_edge_embeddings),
        'embedding_dim': all_center_edge_embeddings.shape[1],
        'score_range': (all_structural_scores.min().item(), all_structural_scores.max().item()),
        'score_mean': all_structural_scores.mean().item(),
        'score_std': all_structural_scores.std().item(),
        'score_median': all_structural_scores.median().item()
    }
    
    print("=== æ¨¡å‹æ€§èƒ½è¯„ä¼° ===")
    print(f"æ€»å­å›¾æ•°: {stats['total_graphs']}")
    print(f"åµŒå…¥ç»´åº¦: {stats['embedding_dim']}")
    print(f"è¾“å‡ºå½¢çŠ¶: {all_center_edge_embeddings.shape}")  # åº”è¯¥æ˜¯ [100, 128]
    print(f"ç»“æ„å¾—åˆ†èŒƒå›´: [{stats['score_range'][0]:.4f}, {stats['score_range'][1]:.4f}]")
    print(f"ç»“æ„å¾—åˆ†å‡å€¼: {stats['score_mean']:.4f}")
    print(f"ç»“æ„å¾—åˆ†æ ‡å‡†å·®: {stats['score_std']:.4f}")
    print(f"ç»“æ„å¾—åˆ†ä¸­ä½æ•°: {stats['score_median']:.4f}")
    
    return all_center_edge_embeddings, all_structural_scores, stats


def save_results(model, center_edge_embeddings, structural_scores, stats, config):
    """ä¿å­˜ç»“æœ"""
    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'edge_embedding_model.pt')
    
    # ä¿å­˜ç»“æœ
    torch.save({
        'center_edge_embeddings': center_edge_embeddings,
        'structural_scores': structural_scores,
        'stats': stats,
        'config': config
    }, 'edge_embedding_results.pt')
    
    # ä¿å­˜é…ç½®
    import json
    with open('model_config.json', 'w') as f:
        json.dump(config, f, indent=2)
    
    print("ç»“æœå·²ä¿å­˜ï¼")
    print(f"ä¸­å¿ƒè¾¹åµŒå…¥å½¢çŠ¶: {center_edge_embeddings.shape}")  # åº”è¯¥æ˜¯ [100, 128]


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¾¹åµŒå…¥æ¨¡å‹è®­ç»ƒ")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # è®¾å¤‡é…ç½®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®è·¯å¾„
    pkl_path = "ego_graphs_20250731_194528/ego_graphs.pkl"
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(pkl_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {pkl_path}")
        return
    
    # 1. æ•°æ®è½¬æ¢
    print("\nğŸ“Š æ­¥éª¤1: æ•°æ®è½¬æ¢")
    graphs = convert_ego_graphs_to_pytorch(pkl_path)
    
    # 2. åˆ›å»ºæ¨¡å‹é…ç½®
    print("\nâš™ï¸ æ­¥éª¤2: åˆ›å»ºæ¨¡å‹é…ç½®")
    config = create_model_config()
    print("æ¨¡å‹é…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # 3. æ•°æ®åˆ’åˆ†
    print("\nğŸ“Š æ­¥éª¤3: æ•°æ®åˆ’åˆ†")
    train_graphs, test_graphs = split_data(graphs, train_ratio=0.8, test_ratio=0.2)
    
    # 4. è®­ç»ƒæ¨¡å‹
    print("\nğŸ¯ æ­¥éª¤4: è®­ç»ƒæ¨¡å‹")
    model, training_history, test_graphs = train_model(
        train_graphs=train_graphs,
        test_graphs=test_graphs,
        config=config,
        num_epochs=500,
        learning_rate=1e-3,
        device=device
    )
    
    # 5. è¯„ä¼°æ¨¡å‹
    print("\nğŸ“ˆ æ­¥éª¤5: è¯„ä¼°æ¨¡å‹")
    center_edge_embeddings, structural_scores, stats = evaluate_model_performance(
        model, test_graphs, device
    )
    
    # 6. ä¿å­˜ç»“æœ
    print("\nğŸ’¾ æ­¥éª¤6: ä¿å­˜ç»“æœ")
    save_results(model, center_edge_embeddings, structural_scores, stats, config)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("- edge_embedding_model.pt (æœ€ç»ˆæ¨¡å‹)")
    print("- best_edge_embedding_model.pt (æœ€ä½³æ¨¡å‹)")
    print("- edge_embedding_results.pt (æ¨ç†ç»“æœ)")
    print("- model_config.json (æ¨¡å‹é…ç½®)")
    print("- training_history.npy (è®­ç»ƒå†å²)")


if __name__ == "__main__":
    main() 