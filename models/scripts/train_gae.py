"""
GAEæ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import os
import sys
import torch
import torch.optim as optim
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple

from gae_model import create_gae_model
from autoencoder_utils import (
    convert_route_graphs_to_pytorch,
    split_data,
    load_config,
    compute_link_prediction_metrics,
    get_pos_neg_edges,
    plot_training_curves,
    print_model_summary
)


def evaluate_gae(
    model: torch.nn.Module,
    graphs: List,
    device: str
) -> Dict[str, float]:
    """
    è¯„ä¼°GAEæ¨¡å‹æ€§èƒ½
    
    Args:
        model: GAEæ¨¡å‹
        graphs: å›¾æ•°æ®åˆ—è¡¨
        device: è®¾å¤‡
        
    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    model.eval()
    
    total_loss = 0.0
    all_pos_preds = []
    all_neg_preds = []
    all_pos_edges = []
    all_neg_edges = []
    
    with torch.no_grad():
        for graph in graphs:
            graph = graph.to(device)
            
            # è·å–æ­£è´Ÿæ ·æœ¬è¾¹
            pos_edge_index, neg_edge_index = get_pos_neg_edges(
                graph.edge_index,
                graph.x.size(0)
            )
            
            # è®¡ç®—æŸå¤±
            loss = model.recon_loss(graph, pos_edge_index, neg_edge_index)
            total_loss += loss.item()
            
            # è·å–é¢„æµ‹
            pos_pred, neg_pred = model.test(graph, pos_edge_index, neg_edge_index)
            
            all_pos_preds.append(pos_pred)
            all_neg_preds.append(neg_pred)
            all_pos_edges.append(pos_edge_index)
            all_neg_edges.append(neg_edge_index)
    
    # åˆå¹¶æ‰€æœ‰é¢„æµ‹
    pos_preds_cat = torch.cat(all_pos_preds)
    neg_preds_cat = torch.cat(all_neg_preds)
    pos_edges_cat = torch.cat(all_pos_edges, dim=1)
    neg_edges_cat = torch.cat(all_neg_edges, dim=1)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = compute_link_prediction_metrics(
        pos_edges_cat,
        neg_edges_cat,
        pos_preds_cat,
        neg_preds_cat
    )
    
    metrics['loss'] = total_loss / len(graphs)
    
    return metrics


def train_gae(config: Dict) -> None:
    """
    è®­ç»ƒGAEæ¨¡å‹
    
    Args:
        config: é…ç½®å­—å…¸
    """
    print("="*60)
    print("GAE æ¨¡å‹è®­ç»ƒ")
    print("="*60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # è®¾å¤‡é…ç½®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“Š æ­¥éª¤1: åŠ è½½æ•°æ®")
    data_path = config['paths']['data']
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return
    
    graphs = convert_route_graphs_to_pytorch(data_path)
    
    # åˆ’åˆ†æ•°æ®é›†
    print("\nğŸ“Š æ­¥éª¤2: åˆ’åˆ†æ•°æ®é›†")
    train_graphs, val_graphs = split_data(
        graphs,
        train_ratio=config['training']['train_ratio'],
        val_ratio=config['training']['val_ratio'],
        seed=config['training']['seed']
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸ æ­¥éª¤3: åˆ›å»ºæ¨¡å‹")
    model = create_gae_model(config['model'])
    model = model.to(device)
    print_model_summary(model, "GAE")
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    print("\nâš™ï¸ æ­¥éª¤4: é…ç½®ä¼˜åŒ–å™¨")
    optimizer = optim.Adam(
        model.parameters(),
        lr=config['training']['learning_rate'],
        weight_decay=config['training']['weight_decay']
    )
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        **config['optimizer']['scheduler_params']
    )
    
    # è®­ç»ƒå¾ªç¯
    print("\nğŸš‚ æ­¥éª¤5: å¼€å§‹è®­ç»ƒ")
    print(f"è®­ç»ƒè½®æ•°: {config['training']['num_epochs']}")
    
    best_val_auc = 0.0
    training_history = {
        'train_loss': [],
        'val_loss': [],
        'val_precision': [],
        'val_accuracy': [],
        'val_auc': [],
        'val_ap': []
    }
    
    for epoch in range(config['training']['num_epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0.0
        
        for graph in train_graphs:
            graph = graph.to(device)
            
            # å‰å‘ä¼ æ’­
            loss = model.recon_loss(graph)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_train_loss += loss.item()
        
        avg_train_loss = total_train_loss / len(train_graphs)
        
        # éªŒè¯é˜¶æ®µ
        val_metrics = evaluate_gae(model, val_graphs, device)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_metrics['loss'])
        
        # è®°å½•å†å²
        training_history['train_loss'].append(avg_train_loss)
        training_history['val_loss'].append(val_metrics['loss'])
        training_history['val_precision'].append(val_metrics['precision'])
        training_history['val_accuracy'].append(val_metrics['accuracy'])
        training_history['val_auc'].append(val_metrics['auc'])
        training_history['val_ap'].append(val_metrics['ap'])
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['auc'] > best_val_auc:
            best_val_auc = val_metrics['auc']
            torch.save(model.state_dict(), config['paths']['best_model_save'])
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 10 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch [{epoch+1}/{config['training']['num_epochs']}] "
                  f"Train Loss: {avg_train_loss:.4f} | "
                  f"Val Loss: {val_metrics['loss']:.4f} | "
                  f"Val Precision: {val_metrics['precision']:.4f} | "
                  f"Val Accuracy: {val_metrics['accuracy']:.4f} | "
                  f"Val AUC: {val_metrics['auc']:.4f} | "
                  f"LR: {current_lr:.6f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nğŸ’¾ æ­¥éª¤6: ä¿å­˜ç»“æœ")
    torch.save(model.state_dict(), config['paths']['model_save'])
    
    # ä¿å­˜è®­ç»ƒå†å²
    np.save(config['paths']['history_save'], training_history)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(
        training_history,
        config['paths']['training_curve'],
        model_name="GAE"
    )
    
    # ç”Ÿæˆå›¾åµŒå…¥
    print("\nğŸ“Š æ­¥éª¤7: ç”Ÿæˆå›¾åµŒå…¥")
    model.eval()
    all_embeddings = []
    all_graph_ids = []
    
    with torch.no_grad():
        for graph in graphs:
            graph = graph.to(device)
            embedding = model.get_graph_embedding(graph)
            all_embeddings.append(embedding.cpu())
            all_graph_ids.append(graph.graph_id)
    
    all_embeddings = torch.cat(all_embeddings, dim=0)
    
    # ä¿å­˜åµŒå…¥
    torch.save({
        'embeddings': all_embeddings,
        'graph_ids': all_graph_ids,
        'config': config
    }, config['paths']['embeddings_save'])
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æœ€ä½³éªŒè¯AUC: {best_val_auc:.4f}")
    print(f"å›¾åµŒå…¥å½¢çŠ¶: {all_embeddings.shape}")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"- {config['paths']['model_save']}")
    print(f"- {config['paths']['best_model_save']}")
    print(f"- {config['paths']['embeddings_save']}")
    print(f"- {config['paths']['training_curve']}")
    print(f"- {config['paths']['history_save']}")


def main() -> None:
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config_path = r"D:\Architecture\AAA-Master\25Fall\CAADRIA\UrbanStreetGNN\models\config\gae_config.json"   
    config = load_config(config_path)
    
    # è®­ç»ƒæ¨¡å‹
    train_gae(config)


if __name__ == "__main__":
    main()

