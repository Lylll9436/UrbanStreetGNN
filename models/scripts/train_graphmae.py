"""
GraphMAEæ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import os
import sys
import io
import logging
import torch
import torch.optim as optim
import numpy as np
from datetime import datetime
from typing import Any, Dict, List, Tuple, Optional
from pathlib import Path
from contextlib import redirect_stdout

from graphmae_model import create_graphmae_model
from autoencoder_utils import (
    convert_route_graphs_to_pytorch,
    split_data,
    load_config,
    compute_link_prediction_metrics,
    get_pos_neg_edges,
    plot_training_curves,
    print_model_summary
)


def setup_logger(subdir: str, enable_file: bool) -> Tuple[logging.Logger, Optional[Path]]:
    """
    åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
    """
    logger = logging.getLogger(f"{subdir}_trainer")
    logger.handlers.clear()
    logger.setLevel(logging.INFO)

    log_path: Optional[Path] = None
    if enable_file:
        base_dir = (Path(__file__).resolve().parent / f"../outputs/logs/{subdir}").resolve()
        base_dir.mkdir(parents=True, exist_ok=True)
        log_path = base_dir / f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_path, encoding='utf-8')
        file_handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        logger.addHandler(file_handler)
    else:
        logger.addHandler(logging.NullHandler())

    return logger, log_path


def evaluate_graphmae(
    model: torch.nn.Module,
    graphs: List,
    device: str,
    link_weight: float
) -> Dict[str, float]:
    """
    è¯„ä¼°GraphMAEæ¨¡å‹æ€§èƒ½
    
    Args:
        model: GraphMAEæ¨¡å‹
        graphs: å›¾æ•°æ®åˆ—è¡¨
        device: è®¾å¤‡
        link_weight: é“¾æ¥æŸå¤±æƒé‡
        
    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    model.eval()
    
    total_loss = 0.0
    total_feature_loss = 0.0
    total_link_loss = 0.0
    all_pos_preds = []
    all_neg_preds = []
    all_pos_edges = []
    all_neg_edges = []
    
    with torch.no_grad():
        for graph in graphs:
            graph = graph.to(device)
            
            # è®¡ç®—æŸå¤±
            loss, loss_dict = model.loss(graph, link_weight)
            total_loss += loss_dict['total']
            total_feature_loss += loss_dict['feature']
            total_link_loss += loss_dict['link']
            
            # è·å–æ­£è´Ÿæ ·æœ¬è¾¹
            pos_edge_index, neg_edge_index = get_pos_neg_edges(
                graph.edge_index,
                graph.x.size(0)
            )
            
            # è·å–é¢„æµ‹
            pos_pred, neg_pred = model.test(graph, pos_edge_index, neg_edge_index)
            
            all_pos_preds.append(pos_pred)
            all_neg_preds.append(neg_pred)
            all_pos_edges.append(pos_edge_index)
            all_neg_edges.append(neg_edge_index)
    
    # åˆå¹¶æ‰€æœ‰é¢„æµ‹
    pos_preds_cat = torch.cat(all_pos_preds)
    neg_preds_cat = torch.cat(all_neg_preds)
    pos_edges_cat = torch.cat(all_pos_edges, dim=1)
    neg_edges_cat = torch.cat(all_neg_edges, dim=1)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = compute_link_prediction_metrics(
        pos_edges_cat,
        neg_edges_cat,
        pos_preds_cat,
        neg_preds_cat
    )
    
    metrics['loss'] = total_loss / len(graphs)
    metrics['feature_loss'] = total_feature_loss / len(graphs)
    metrics['link_loss'] = total_link_loss / len(graphs)
    
    return metrics


def train_graphmae(config: Dict, trial_mode: bool = False) -> Dict[str, Any]:
    """
    è®­ç»ƒGraphMAEæ¨¡å‹
    
    Args:
        config: é…ç½®å­—å…¸
    """
    logger, log_path = setup_logger("graphmae", enable_file=not trial_mode)

    def log_info(message: str) -> None:
        logger.info(message)
        print(message)

    if log_path is not None:
        log_info(f"æ—¥å¿—æ–‡ä»¶: {log_path}")

    if not trial_mode:
        log_info("="*60)
        log_info("GraphMAE æ¨¡å‹è®­ç»ƒ")
        log_info("="*60)
        log_info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # è®¾å¤‡é…ç½®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    if not trial_mode:
        log_info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    if not trial_mode:
        log_info("\nğŸ“Š æ­¥éª¤1: åŠ è½½æ•°æ®")
    data_path = config['paths']['data']
    if not os.path.exists(data_path):
        log_info(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return {"best_val_auc": 0.0, "training_history": {}}
    
    graphs = convert_route_graphs_to_pytorch(data_path)
    
    # åˆ’åˆ†æ•°æ®é›†
    if not trial_mode:
        log_info("\nğŸ“Š æ­¥éª¤2: åˆ’åˆ†æ•°æ®é›†")
    train_graphs, val_graphs = split_data(
        graphs,
        train_ratio=config['training']['train_ratio'],
        val_ratio=config['training']['val_ratio'],
        seed=config['training']['seed']
    )
    
    # åˆ›å»ºæ¨¡å‹
    if not trial_mode:
        log_info("\nğŸ—ï¸ æ­¥éª¤3: åˆ›å»ºæ¨¡å‹")
    model = create_graphmae_model(config['model'])
    model = model.to(device)
    if not trial_mode and log_path is not None:
        summary_buffer = io.StringIO()
        with redirect_stdout(summary_buffer):
            print_model_summary(model, "GraphMAE")
        for line in summary_buffer.getvalue().splitlines():
            log_info(line)
    else:
        print_model_summary(model, "GraphMAE")
    if not trial_mode:
        log_info(f"æ©ç æ¯”ä¾‹: {config['model']['mask_ratio']}")
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    if not trial_mode:
        log_info("\nâš™ï¸ æ­¥éª¤4: é…ç½®ä¼˜åŒ–å™¨")
    optimizer = optim.Adam(
        model.parameters(),
        lr=config['training']['learning_rate'],
        weight_decay=config['training']['weight_decay']
    )
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        **config['optimizer']['scheduler_params']
    )
    
    # è®­ç»ƒå¾ªç¯
    if not trial_mode:
        log_info("\nğŸš‚ æ­¥éª¤5: å¼€å§‹è®­ç»ƒ")
        log_info(f"è®­ç»ƒè½®æ•°: {config['training']['num_epochs']}")

    def display_epoch_progress(epoch_idx: int,
                               total_epochs: int,
                               train_loss_value: float,
                               val_metrics_value: Dict[str, float],
                               link_weight_value: float) -> None:
        bar_len = 30
        progress = (epoch_idx + 1) / total_epochs
        filled = int(bar_len * progress)
        bar = 'â–ˆ' * filled + '-' * (bar_len - filled)
        base_msg = (f"[{bar}] Epoch {epoch_idx + 1}/{total_epochs} "
                    f"Train Loss: {train_loss_value:.4f} | Link_w: {link_weight_value:.4f}")

        def _fmt(metric_value: float) -> str:
            return "N/A" if metric_value is None or np.isnan(metric_value) else f"{metric_value:.4f}"

        base_msg += (f" | Val Loss: {_fmt(val_metrics_value.get('loss'))} "
                     f"| Feature: {_fmt(val_metrics_value.get('feature_loss'))} "
                     f"| Link: {_fmt(val_metrics_value.get('link_loss'))} "
                     f"| AUC: {_fmt(val_metrics_value.get('auc'))}")
        if not trial_mode:
            log_info(base_msg)
    
    link_weight = config['training']['link_weight']

    last_val_metrics = {
        'loss': float('nan'),
        'feature_loss': float('nan'),
        'link_loss': float('nan'),
        'precision': float('nan'),
        'accuracy': float('nan'),
        'auc': float('nan'),
        'ap': float('nan')
    }
    
    best_val_auc = 0.0
    training_history = {
        'train_loss': [],
        'val_loss': [],
        'val_feature_loss': [],
        'val_link_loss': [],
        'val_precision': [],
        'val_accuracy': [],
        'val_auc': [],
        'val_ap': []
    }
    
    for epoch in range(config['training']['num_epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0.0
        
        for graph in train_graphs:
            graph = graph.to(device)
            
            # å‰å‘ä¼ æ’­
            loss, loss_dict = model.loss(graph, link_weight)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_train_loss += loss_dict['total']
        
        avg_train_loss = total_train_loss / len(train_graphs)
        
        val_metrics = evaluate_graphmae(model, val_graphs, device, link_weight)
        last_val_metrics = val_metrics
        scheduler.step(val_metrics['loss'])
        if val_metrics['auc'] > best_val_auc:
            best_val_auc = val_metrics['auc']
            torch.save(model.state_dict(), config['paths']['best_model_save'])
        
        # è®°å½•å†å²
        training_history['train_loss'].append(avg_train_loss)
        training_history['val_loss'].append(val_metrics['loss'])
        training_history['val_feature_loss'].append(val_metrics['feature_loss'])
        training_history['val_link_loss'].append(val_metrics['link_loss'])
        training_history['val_precision'].append(val_metrics['precision'])
        training_history['val_accuracy'].append(val_metrics['accuracy'])
        training_history['val_auc'].append(val_metrics['auc'])
        training_history['val_ap'].append(val_metrics['ap'])

        display_epoch_progress(epoch,
                               config['training']['num_epochs'],
                               avg_train_loss,
                               val_metrics,
                               link_weight)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if not trial_mode:
        log_info("\nğŸ’¾ æ­¥éª¤6: ä¿å­˜ç»“æœ")
        torch.save(model.state_dict(), config['paths']['model_save'])
        
        # ä¿å­˜è®­ç»ƒå†å²
        np.save(config['paths']['history_save'], training_history)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        plot_training_curves(
            training_history,
            config['paths']['training_curve'],
            model_name="GraphMAE"
        )
        
        # ç”Ÿæˆå›¾åµŒå…¥
        log_info("\nğŸ“Š æ­¥éª¤7: ç”Ÿæˆå›¾åµŒå…¥")
        model.eval()
        all_embeddings = []
        all_graph_ids = []
        
        with torch.no_grad():
            for graph in graphs:
                graph = graph.to(device)
                embedding = model.get_graph_embedding(graph)
                all_embeddings.append(embedding.cpu())
                all_graph_ids.append(graph.graph_id)
        
        all_embeddings = torch.cat(all_embeddings, dim=0)
        
        # ä¿å­˜åµŒå…¥
        torch.save({
            'embeddings': all_embeddings,
            'graph_ids': all_graph_ids,
            'config': config
        }, config['paths']['embeddings_save'])
        
        log_info(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
        log_info(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        log_info(f"æœ€ä½³éªŒè¯AUC: {best_val_auc:.4f}")
        log_info(f"å›¾åµŒå…¥å½¢çŠ¶: {all_embeddings.shape}")
        log_info("\nç”Ÿæˆçš„æ–‡ä»¶:")
        log_info(f"- {config['paths']['model_save']}")
        log_info(f"- {config['paths']['best_model_save']}")
        log_info(f"- {config['paths']['embeddings_save']}")
        log_info(f"- {config['paths']['training_curve']}")
        log_info(f"- {config['paths']['history_save']}")
    else:
        if not np.isnan(last_val_metrics.get('loss', float('nan'))):
            log_info(f"[Trial] æœ€ä½³éªŒè¯AUC: {best_val_auc:.4f}")

    return {
        "best_val_auc": best_val_auc,
        "best_val_metrics": last_val_metrics,
        "training_history": training_history
    }


def main() -> None:
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    current_dir = Path(__file__).resolve().parent
    config_path = current_dir / "../config/graphmae_config.json"
    config = load_config(config_path)
    
    # è®­ç»ƒæ¨¡å‹
    train_graphmae(config)


if __name__ == "__main__":
    main()
